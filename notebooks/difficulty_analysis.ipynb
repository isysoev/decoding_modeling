{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "difficulty analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "SNXr1glZahgl"
      ],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "viVudzm08qHT",
        "outputId": "9a3a8dcb-8627-490a-acc0-b774aa1ac5d3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('./drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at ./drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQaLvAaO7bnB"
      },
      "source": [
        "import pymc3 as pm\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch.distributions import * \n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.stats as scistats\n",
        "from scipy.special import binom\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "from functools import reduce\n",
        "import seaborn as sb\n",
        "\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZAvrsri8i4G"
      },
      "source": [
        "DATA_PATH = './drive/MyDrive/urop/wong_urop/model/data'\n",
        "import os\n",
        "import h5py\n",
        "from os.path import join, exists"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYOtwHPi_DqZ"
      },
      "source": [
        "#Data loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2l1CYLH7bnC"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import codecs\n",
        "\n",
        "def get_basic_phoneme(phoneme):\n",
        "\tif phoneme[-1].isdigit():\n",
        "\t\treturn phoneme[:-1]\n",
        "\treturn phoneme\n",
        "\n",
        "def get_phonemes(phonemes_code):\n",
        "\treturn tuple([get_basic_phoneme(phoneme_code) for phoneme_code in phonemes_code.split(';')])\n",
        "\n",
        "def get_pg_pair(pg_pair_code):\n",
        "\tphonemes_code, grapheme = pg_pair_code.split('>')\n",
        "\treturn (get_phonemes(phonemes_code), grapheme)\n",
        "\n",
        "def get_mapping(mapping_code):\n",
        "\treturn tuple([get_pg_pair(pg_pair_code) for pg_pair_code in mapping_code.split('|')])\n",
        "\n",
        "def read_phonix(input_file_name):\n",
        "\tphonix = []\n",
        "\twith codecs.open(input_file_name, encoding = 'utf-8') as input_file:\n",
        "\t\tfor line in input_file:\n",
        "\t\t\tline = line.strip()\n",
        "\t\t\tif not line: continue\n",
        "\t\t\tword, mapping_code = line.split(' ')\n",
        "\t\t\tphonix.append((word, get_mapping(mapping_code)))\n",
        "\treturn phonix\n",
        "\n",
        "def pg_pair_to_str(pg_pair):\n",
        "\tphonemes, grapheme = pg_pair\n",
        "\treturn '%s>%s' % (';'.join(phonemes), grapheme)\n",
        "\n",
        "def mapping_to_str(mapping):\n",
        "\treturn '|'.join(pg_pair_to_str(pg_pair) for pg_pair in mapping)\n",
        " \n",
        "def read_freq_list(freq_file_name):\n",
        "    wordfreqs = {}\n",
        "    with open(freq_file_name) as input_file:\n",
        "        for line in input_file:\n",
        "            line = line.strip()\n",
        "            word, freq = line.split(' ')\n",
        "            freq = float(freq)\n",
        "            wordfreqs[word] = freq\n",
        "    return wordfreqs\n",
        "\n",
        "def get_pg_freqs(wordfreqs, phonix):\n",
        "    aggregator = defaultdict(float)\n",
        "    for word, mapping in phonix:\n",
        "        if word not in wordfreqs: continue\n",
        "        wordfreq = wordfreqs[word]\n",
        "        for pg in mapping:\n",
        "            aggregator[pg_pair_to_str(pg)] += wordfreq\n",
        "    return normalize(aggregator)\n",
        "\n",
        "def word_pgs(word):\n",
        "    return [pg_pair_to_str(pg) for pg in phonix_dict[word]]\n",
        "    \n",
        "def observed_pgs(observations):\n",
        "    pgs = set()\n",
        "    for word, obs in observations:\n",
        "        pgs.update(word_pgs(word))\n",
        "    return pgs"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLQ2BAPM7bnC"
      },
      "source": [
        "def normalize(distr):\n",
        "    denominator = sum(distr.values())\n",
        "    return {key: float(value) / denominator for key, value in distr.items()}"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTZGgcd_7bnC"
      },
      "source": [
        "phonix = read_phonix(join(DATA_PATH, 'phonix.txt'))\n",
        "phonix_dict = dict(phonix)\n",
        "wordfreqs = read_freq_list(join(DATA_PATH, 'word-freqs.txt'))\n",
        "\n",
        "word_list = sorted(list(set(phonix_dict.keys()) & set(wordfreqs.keys() - {'null', 'nan'}))) \n",
        "#Above is because of strange behaviors with DF.\n",
        "\n",
        "pg_freqs = get_pg_freqs(wordfreqs, phonix)\n",
        "\n",
        "pgs = sorted(pg_freqs.keys(), key = lambda pg: pg_freqs[pg], reverse=True)\n",
        "ps = np.array([pg_freqs[pg] for pg in pgs])\n",
        "\n",
        "pg_idx = {pg : i for i, pg in enumerate(pgs)}"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lFWjVQaVBUQ"
      },
      "source": [
        "def gen_word2pg_counts(word_list, pg_idx):\n",
        "  #Some code taken from Ivan's.\n",
        "\n",
        "  counts = []\n",
        "  for word in word_list:\n",
        "\n",
        "    #Identify idx of the pg pairs in word.\n",
        "    the_word_pgs = word_pgs(word)\n",
        "    the_word_pgs_idx = np.array([pg_idx[pg] for pg in the_word_pgs])\n",
        "\n",
        "    #Convert to one-hot like vector of counts\n",
        "    this_counts = np.bincount(the_word_pgs_idx, minlength = len(pg_idx))\n",
        "    counts.append(this_counts)\n",
        "\n",
        "  return np.stack(counts)\n",
        "\n",
        "word2pg_counts = torch.from_numpy(gen_word2pg_counts(word_list, pg_idx)).cuda()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ky89WdfO_H5c"
      },
      "source": [
        "#Difficulty computation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URecs1MOk9gH"
      },
      "source": [
        "#12/7: https://www.tensorflow.org/probability\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Q5-m8BZ7bnD"
      },
      "source": [
        "def _probs_to_word_prob(probs_recall, word2pg_counts_arr):\n",
        "\n",
        "    #Note that if the count for a given pg is zero, the result is 1,\n",
        "    #  which doesn't affect the product as expected.\n",
        "    raw_products = torch.pow(probs_recall, word2pg_counts_arr) #Check idx ordering?\n",
        "    p_pg = torch.prod(raw_products, axis = 1) #Collapse pg, retain words.\n",
        "  \n",
        "    return p_pg\n",
        "    \n",
        "def simulate_child(word_list, prior_mu = np.log(500), prior_sigma = 3):\n",
        "\n",
        "    lr = 0.3\n",
        "\n",
        "    #12/7: https://stackoverflow.com/questions/34097281/convert-a-tensor-to-numpy-array-in-tensorflow\n",
        "    log_n = tfp.distributions.TruncatedNormal(loc = prior_mu, scale = prior_sigma, low = 0, high = float('inf')).sample()\n",
        "    n = int(np.exp(log_n.numpy())) \n",
        "\n",
        "    n_pg_raw = torch.multinomial(torch.from_numpy(ps).cuda(), n, replacement=True) #Fictional counts only.\n",
        "    n_pg = torch.bincount(n_pg_raw, minlength = ps.shape[0])\n",
        "    \n",
        "    assert torch.sum(n_pg) == n, 'Allocation not correct.'\n",
        "    \n",
        "    #Simulate recalls.\n",
        "    probs_recall = 1 - torch.exp(-lr * n_pg)\n",
        "    words_probs = _probs_to_word_prob(probs_recall, word2pg_counts_arr = word2pg_counts)\n",
        "    recalled = torch.bernoulli(words_probs)\n",
        "    return recalled"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMQjQIkeA4n-"
      },
      "source": [
        "def repeat_simulation(word_list, trials):\n",
        "\n",
        "  aggregate = torch.zeros((len(word_list),)).cuda()\n",
        "\n",
        "  for trial_num in range(trials):\n",
        "    if trial_num % 100 == 0:\n",
        "      print(f'Trial: {trial_num}')\n",
        "    this_child_recall = simulate_child(word_list)\n",
        "    aggregate += this_child_recall\n",
        "\n",
        "  difficulty = aggregate / trials\n",
        "  return difficulty"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Cgto-u5Eul1",
        "outputId": "e9db1ff0-5e86-4fde-ee30-dce16e5ffc70"
      },
      "source": [
        "difficulty = repeat_simulation(word_list, 5000) \n",
        "\n",
        "difficulty_df = pd.DataFrame.from_dict({\n",
        "    'words': word_list,\n",
        "    'difficulty': difficulty.tolist()\n",
        "})\n",
        "\n",
        "difficulty_df['words'].astype('str')\n",
        "\n",
        "save_folder = join(DATA_PATH, 'difficulty')\n",
        "#save_folder = os.getcwd()\n",
        "\n",
        "if not os.path.exists(save_folder):\n",
        "  os.makedirs(save_folder)\n",
        "  \n",
        "save_difficulty_path = join(save_folder, 'difficulty.csv')\n",
        "difficulty_df.to_csv(save_difficulty_path)\n",
        "\n",
        "print(f'Difficulty written to {save_difficulty_path}')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trial: 0\n",
            "Trial: 100\n",
            "Trial: 200\n",
            "Trial: 300\n",
            "Trial: 400\n",
            "Trial: 500\n",
            "Trial: 600\n",
            "Trial: 700\n",
            "Trial: 800\n",
            "Trial: 900\n",
            "Trial: 1000\n",
            "Trial: 1100\n",
            "Trial: 1200\n",
            "Trial: 1300\n",
            "Trial: 1400\n",
            "Trial: 1500\n",
            "Trial: 1600\n",
            "Trial: 1700\n",
            "Trial: 1800\n",
            "Trial: 1900\n",
            "Trial: 2000\n",
            "Trial: 2100\n",
            "Trial: 2200\n",
            "Trial: 2300\n",
            "Trial: 2400\n",
            "Trial: 2500\n",
            "Trial: 2600\n",
            "Trial: 2700\n",
            "Trial: 2800\n",
            "Trial: 2900\n",
            "Trial: 3000\n",
            "Trial: 3100\n",
            "Trial: 3200\n",
            "Trial: 3300\n",
            "Trial: 3400\n",
            "Trial: 3500\n",
            "Trial: 3600\n",
            "Trial: 3700\n",
            "Trial: 3800\n",
            "Trial: 3900\n",
            "Trial: 4000\n",
            "Trial: 4100\n",
            "Trial: 4200\n",
            "Trial: 4300\n",
            "Trial: 4400\n",
            "Trial: 4500\n",
            "Trial: 4600\n",
            "Trial: 4700\n",
            "Trial: 4800\n",
            "Trial: 4900\n",
            "Difficulty written to ./drive/MyDrive/urop/wong_urop/model/data/difficulty/difficulty.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3MyaghcGUE2",
        "outputId": "8f8c3458-c63a-4436-938d-5f1258da4943"
      },
      "source": [
        "#Verification.\n",
        "\n",
        "#Difference in the difficulties\n",
        "\n",
        "arr_diff = np.array(difficulty_df)\n",
        "arr_load = np.array(pd.read_csv(save_difficulty_path))\n",
        "print('Sum difference in difficulty scores', np.sum(arr_diff[:, 1] - arr_load[:, 2])) #arr_load includes an extra 'index' column\n",
        "\n",
        "#Words matching? This is to ensure that 'null' and 'nan' were removed from the words.\n",
        "assert (difficulty_df.iloc[:, 0] == pd.read_csv(save_difficulty_path).iloc[:, 1]).all()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sum difference in difficulty scores -5.98438399529444e-14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNXr1glZahgl"
      },
      "source": [
        "#Tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XXQZn5FXbfV"
      },
      "source": [
        "def test_gen_word2pg_counts():\n",
        "  test_word_list = ['can', 'cactus']\n",
        "\n",
        "  #Manually from pg_idx and phonix_dict:\n",
        "  #'ʌ>a' -> 15\n",
        "  #'k>c' -> 22\n",
        "  #'n>n' -> 1\n",
        "  #'æ>a' -> 4\n",
        "  #'t>t' -> 0\n",
        "  #'ʌ>u' -> 21\n",
        "  #'s>s' -> 5\n",
        "\n",
        "  actual = gen_word2pg_counts(test_word_list, pg_idx)\n",
        "  list_where_one = [[22, 15, 1], [22, 4, 0, 21, 5]]\n",
        "\n",
        "  expected = []\n",
        "  for where_one in list_where_one:\n",
        "    this_expected = np.zeros(len(pg_idx))\n",
        "    this_expected[where_one] = 1\n",
        "    expected.append(this_expected)\n",
        "\n",
        "  expected[1][22] = 2 #Two k>c in \"cactus\".\n",
        "  expected = np.stack(expected)\n",
        "\n",
        "  assert np.all(actual == expected), 'word2pgcounts function check failed.'"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcLLGx0uedXJ"
      },
      "source": [
        "def test_probs_to_word_prob():\n",
        "  test_recall_probs = torch.from_numpy(np.array([2., 1., 3.]))\n",
        "  test_counts = torch.from_numpy(np.array([\n",
        "                          [0, 3, 2],\n",
        "                          [1, 1, 0],\n",
        "                          [3, 1, 2]\n",
        "  ]))\n",
        "\n",
        "  expected = torch.Tensor([9, 2, 72])\n",
        "  actual = _probs_to_word_prob(test_recall_probs, test_counts)\n",
        "\n",
        "  assert torch.all(expected == actual), 'probs to word prob test failed'"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RswttDWPedvf",
        "outputId": "8f0964eb-ef01-4eee-c951-34d65268dc74"
      },
      "source": [
        "tests = [\n",
        "         test_gen_word2pg_counts,\n",
        "         test_probs_to_word_prob\n",
        "]\n",
        "\n",
        "for test in tests:\n",
        "  test()\n",
        "\n",
        "print('Tests passed.')"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tests passed.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}