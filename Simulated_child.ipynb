{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Simulated child.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "MQqY2xdKrnUv",
        "xn_VSp4MHBmT"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUkEJakCiYcn"
      },
      "source": [
        "!pip install pyro-ppl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8qKTGNSzHv6"
      },
      "source": [
        "#Simulated child"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKG83spt3b-7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('./drive') #This will request authentication to be able to load data."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9ASStjnmL2C"
      },
      "source": [
        "#Seed imports (important that this is run only once per runtime)\n",
        "import pyro\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "SEED = 0\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "#11/21: http://docs.pyro.ai/en/stable/distributions.html?highlight=uniform#uniform\n",
        "import torch.distributions\n",
        "from torch.distributions.uniform import Uniform \n",
        "from torch.distributions.log_normal import LogNormal\n",
        "from torch.distributions.normal import Normal\n",
        "\n",
        "import pyro.distributions\n",
        "from pyro.distributions import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQ5gXqf-EWLh"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FJT1tmn4C2V"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from os.path import join\n",
        "import sys\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cl8Y-3Z6Nwh"
      },
      "source": [
        "DATA_FOLDER = './drive/My Drive/urop/wong_urop/model'\n",
        "#This will have to be changed depending on where your data is stored.\n",
        "DATA_PATH = join(DATA_FOLDER, 'popular_words_shift.csv')\n",
        "\n",
        "COUNTS_FOLDER = join(DATA_FOLDER, 'counts')\n",
        "\n",
        "#11/8: managing the imports\n",
        "#https://stackoverflow.com/questions/4383571/importing-files-from-different-folder\n",
        "import sys\n",
        "sys.path.insert(1, COUNTS_FOLDER)\n",
        "import word2counts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQqY2xdKrnUv"
      },
      "source": [
        "#Old structure (Torch/class): Data and simulated child"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZM7rvor3MZK"
      },
      "source": [
        "class PGData():\n",
        "  \"\"\"\n",
        "  Stores and asks for data from the PG pairs.\n",
        "  \"\"\"\n",
        "  def __init__(self, data_path, data_folder):\n",
        "    \"\"\"\n",
        "    Assumes that the data-related calculations stored\n",
        "       as calculated by the word2counts.py file.\n",
        "\n",
        "    data_path (str) should point to the main words CSV\n",
        "    data_folder (str) should point to the \"counts\" folder.\n",
        "    \"\"\"\n",
        "    #Indexing for pg is determined by this function.\n",
        "    self.pg_idx = np.load(join(data_folder, 'pg_idx.npy'))\n",
        "    #The indexing here is (by words (parallel to main CSV), by pg pair).\n",
        "\n",
        "    #TODO: This is currently just calculating the sum\n",
        "    #     over previously calculated word -> pg pair counts.\n",
        "    #Will change to direct calculation of pg pairs later.\n",
        "    word_pg_counts = np.load(join(data_folder, 'counts.npy'))\n",
        "    self.freqs = torch.from_numpy(np.sum(word_pg_counts, axis = 0)).float()\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.pg_idx.shape[0]\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    \"\"\"\n",
        "    Gives the pg str and count.\n",
        "    \"\"\"\n",
        "    return (self.pg_idx[idx], self.freqs[idx])\n",
        "\n",
        "  def sample_and_extract(self, num_samples):\n",
        "    \"\"\"\n",
        "    Samples num_samples (int) words according the pg probabilities\n",
        "      and returns the vector of counts.\n",
        "    \"\"\"\n",
        "    this_words_idx = torch.multinomial(self.freqs, num_samples)\n",
        "    this_words_counts = torch.bincount(this_words_idx, minlength = len(self))\n",
        "    \n",
        "    return this_words_counts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmQxQOTjmiv3"
      },
      "source": [
        "class Child():\n",
        "  \"\"\"\n",
        "  Represents a child.\n",
        "    Translates and is based on Nikasha's WebPPL program.\n",
        "  Attributes:\n",
        "    (1) lr, the learning rate of a child (alpha)\n",
        "    (2) fictional_exposure_distr, distribution\n",
        "    (3) data, a pointer to the main words CSV file.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, data):\n",
        "    \"\"\"\n",
        "    Inputs: data, a PGData.\n",
        "    \"\"\"\n",
        "    self.data = data\n",
        "    self.lr = Uniform(0, 1).sample()\n",
        "    #TODO: Ask which mean, stdev\n",
        "    mean = 50\n",
        "    self.fictional_exposure_distr = LogNormal(np.log(mean), 0.1)\n",
        "    #Indexing is in order of that of pg_idx.npy\n",
        "\n",
        "  def learn_pg(self):\n",
        "    \"\"\"\n",
        "    Simulates one instance of exposure to pg pairs.\n",
        "    Returns a Tensor of effective exposures over all pg pairs.\n",
        "    \"\"\"\n",
        "    #Sample and \"learn\" new words\n",
        "\n",
        "    this_fictional_exposure = round(self.fictional_exposure_distr.sample().item())\n",
        "    fictional_exposures = self.data.sample_and_extract(this_fictional_exposure)\n",
        "\n",
        "    #Now, return the actual non-deterministic exposure\n",
        "    this_effective_exposures = self.give_effective_exposure(fictional_exposures)\n",
        "    return this_effective_exposures\n",
        "\n",
        "  def give_effective_exposure(self, fictional_exposures):\n",
        "    \"\"\"\n",
        "    Returns Tensor, the effective exposure for each pg pair\n",
        "      based on fictional_exposures, a Tensor\n",
        "    \"\"\"\n",
        "    stdev = 0.1 * torch.ones(len(self.data))\n",
        "    this_exposures = torch.normal(fictional_exposures.float(), stdev)\n",
        "    return this_exposures"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xn_VSp4MHBmT"
      },
      "source": [
        "#Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQU21KzrHAqi"
      },
      "source": [
        "MAIN_DATA = PGData(DATA_PATH, COUNTS_FOLDER)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDUj78XbCAe4"
      },
      "source": [
        "#New structure: Pyro-based"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "My3nfJqPCFU5"
      },
      "source": [
        "def renormalized_probs(raw_freqs):\n",
        "  \"\"\"\n",
        "  Returns the sequentially re-normalized probabilities,\n",
        "    from left to right.\n",
        "  Accepts:\n",
        "    raw_freqs, a Numpy array of raw probabilities.\n",
        "  Returns:\n",
        "    new_freqs, a Torch Tensor\n",
        "      where the probability at index i\n",
        "        is the rennormalized probability for raw_freqs[i:].\n",
        "  \"\"\"\n",
        "\n",
        "  new_freqs = np.zeros(raw_freqs.shape)\n",
        "  \n",
        "  for which_pg_idx in range(raw_freqs.shape[0]):\n",
        "    this_raw_prob = raw_freqs[which_pg_idx]\n",
        "    this_raw_distr = raw_freqs[which_pg_idx:]\n",
        "    prob_sum = np.sum(this_raw_distr)\n",
        "\n",
        "    new_freqs[which_pg_idx] = this_raw_prob / prob_sum\n",
        "\n",
        "  return torch.from_numpy(new_freqs)\n",
        "  \n",
        "#For the PGSampler, I built around and rearranged Ivan's code.\n",
        "\n",
        "class PGSampler():\n",
        "\n",
        "  def __init__(self):\n",
        "    \"\"\"\n",
        "    Assumes that the data-related calculations stored\n",
        "       as calculated by the word2counts.py file.\n",
        "\n",
        "    data_path (str) should point to the main words CSV\n",
        "    data_folder (str) should point to the \"counts\" folder.\n",
        "    \"\"\"\n",
        "    #Indexing for pg is determined by this function.\n",
        "    self.pg_idx = np.load(join(data_folder, 'pg_idx.npy'))\n",
        "    #The indexing here is (by words (parallel to main CSV), by pg pair).\n",
        "\n",
        "    #TODO: This is currently just calculating the sum\n",
        "    #     over previously calculated word -> pg pair counts.\n",
        "    #Will change to direct calculation of pg pairs later.\n",
        "    word_pg_counts = np.load(join(data_folder, 'counts.npy'))\n",
        "\n",
        "    self.raw_freqs = torch.from_numpy(np.sum(word_pg_counts, axis = 0)).float()\n",
        "    self.renorm_freqs = renormalized_probs(self.raw_freqs)\n",
        "\n",
        "    self.num_exposed_distr = pyro.distributions.Normal(np.log(500), 0.1)\n",
        "    self.NUM_PG_PAIRS = self.pg_idx.shape[0]\n",
        "\n",
        "  def allocate_fictional_counts(n_total, observations):\n",
        "    \"\"\"\n",
        "    Allocates pg pair fictional exposure in sample storage and gives array.\n",
        "    Accepts:\n",
        "      n_total, int, the number of pg pairs total.\n",
        "      observations, the observations to condition on.\n",
        "        (Input ignored for now.)\n",
        "    Returns: \n",
        "      all_n_pg_exposed, a Numpy array with the counts per pg pair.\n",
        "        Indices parallel to pg_idx.\n",
        "\n",
        "      Also stores the following samples:\n",
        "          (1) log_n_pg_exposed = the total number of words to which child exposed\n",
        "          (2) {this_pg}_n_fictional_exposed = the exposures to the str {this_pg}, a pg pair.\n",
        "    \"\"\"\n",
        "    #Binomial cascade. TODO: Ask if you had right concept.\n",
        "    which_pg_idx = 0\n",
        "    n = n_total #Remaining pg to allocate between Binomials.\n",
        "\n",
        "    all_n_pg_exposed = np.zeros(self.NUM_PG_PAIRS)\n",
        "\n",
        "    test_pg_counts = []\n",
        "    #Allocate counts to pg pairs.\n",
        "    while n_remaining != 0:\n",
        "      p_pg = self.renorm_freqs[which_pg_idx]\n",
        "      this_pg = self.pg_idx[which_pg_idx]\n",
        "\n",
        "      # this is a model that replaces the binomial with its continuous approximation (from Central Limit Theorem)\n",
        "      n_pg_exposed_distr = pyro.distributions.Normal(loc = n * p_pg, scale = math.sqrt(n * p_pg * (1 - p_pg)))\n",
        "      n_pg_exposed = pyro.sample(f\"{this_pg}_n_fictional_exposed\", n_pg_exposed_distr)\n",
        "\n",
        "      test_pg_counts.append(n_pg_exposed)\n",
        "      #Updates\n",
        "      n -= n_pg_exposed #Subtract the counts allocated to this pg pair.\n",
        "      all_n_pg_exposed[which_pg_idx] = n_pg_exposed\n",
        "      which_pg_idx += 1\n",
        "\n",
        "    assert np.array(test_pg_counts) == all_n_pg_exposed,\\\n",
        "     \"Did not store pg counts correctly.\"\n",
        "     \n",
        "    assert np.sum(all_n_pg_exposed) == n_total,\\\n",
        "     \"Did not allocate pg counts correctly -- sum does not match total number of counts.\"\n",
        "\n",
        "    return all_n_pg_exposed\n",
        "\n",
        "  def calc_effective_counts(all_fictional_exposures):\n",
        "    \"\"\"\n",
        "    Calculates and stores effective exposures.\n",
        "    Inputs:\n",
        "      all_fictional_exposures, a Numpy array.\n",
        "    Returns:\n",
        "      Stores all_fictional_exposures under effective_exposures\n",
        "      n_pg_exposed, a Torch Tensor, the effective counts.\n",
        "    \"\"\"\n",
        "\n",
        "    #TODO: Ask about the stdev here.\n",
        "    #TODO: Ask about if this is correct usage of expectation sum.\n",
        "    stdev = 0.1 * torch.ones(all_fictional_exposures.shape)\n",
        "    n_pg_exposed_distr = pyro.distributions.Normal(loc = all_fictional_exposures,\\\n",
        "                                                   scale = stdev)\n",
        "    \n",
        "    n_pg_exposed = pyro.sample(\"effective_exposures\", n_pg_exposed_distr)\n",
        "    return n_pg_exposed\n",
        "\n",
        "  def approximate_model(observations):\n",
        "    \"\"\"\n",
        "    Samples an total exposure number and exposure numbers for each PG pair.\n",
        "    Accepts:\n",
        "      observations, the fictional exposure observations to condition on.\n",
        "        (Input ignored for now.)\n",
        "    Returns: \n",
        "      all_effective_exposures, a Numpy array with the effective counts per pg pair.\n",
        "        Indices parallel to pg_idx.\n",
        "\n",
        "      Also stores the following samples:\n",
        "          (1) log_n_pg_exposed = the total number of words to which child exposed\n",
        "          (2) {this_pg}_n_fictional_exposed = the exposures to the str {this_pg}, a pg pair.\n",
        "    \"\"\"\n",
        "\n",
        "    log_n = pyro.sample(\"log_n_pg_exposed\", self.num_exposed_distr)\n",
        "    n_total = torch.exp(log_n)\n",
        "\n",
        "    all_fictional_exposures = allocate_fictional_counts(n_total, observations)\n",
        "    all_effective_exposures = calc_effective_counts(all_fictional_exposures)\n",
        "\n",
        "    return all_effective_exposures"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8PHuH98GvSQ"
      },
      "source": [
        "#Verifications"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaBUJY5pGv_L"
      },
      "source": [
        "def check_renormalized_prob():\n",
        "  test_freqs = np.array([1, 0, 0, 7, 3])\n",
        "  expected_renorm = np.array([1/11, 0, 0, 0.7, 1])\n",
        "\n",
        "  actual_renorm = renormalized_probs(test_freqs)\n",
        "\n",
        "  #Note: this test does not account for float behavior,\n",
        "  #   so false positives for test failures may occur,\n",
        "  #   but it should detect unexpected behavior.\n",
        "  assert np.all(expected_renorm == actual_renorm.numpy()),\\\n",
        "   'Rennormalization expected behavior failed.' \n",
        "\n",
        "def run_tests():\n",
        "  tests = [\n",
        "           check_renormalized_prob\n",
        "  ]\n",
        "\n",
        "  for test in tests:\n",
        "    test()\n",
        "\n",
        "run_tests()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}