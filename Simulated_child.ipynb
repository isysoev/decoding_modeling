{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Simulated child.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUkEJakCiYcn"
      },
      "source": [
        "!pip install pyro-ppl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8qKTGNSzHv6"
      },
      "source": [
        "#Simulated child"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKG83spt3b-7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('./drive') #This will request authentication to be able to load data."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9ASStjnmL2C"
      },
      "source": [
        "#Seed imports (important that this is run only once per runtime)\n",
        "import pyro\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "SEED = 0\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "#11/21: http://docs.pyro.ai/en/stable/distributions.html?highlight=uniform#uniform\n",
        "import torch.distributions\n",
        "from torch.distributions.uniform import Uniform \n",
        "from torch.distributions.log_normal import LogNormal\n",
        "from torch.distributions.categorical import Categorical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FJT1tmn4C2V"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from os.path import join\n",
        "import sys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cl8Y-3Z6Nwh"
      },
      "source": [
        "DATA_FOLDER = './drive/My Drive/urop/wong_urop/model'\n",
        "#This will have to be changed depending on where your data is stored.\n",
        "DATA_PATH = join(DATA_FOLDER, 'popular_words_shift.csv')\n",
        "\n",
        "COUNTS_FOLDER = join(DATA_FOLDER, 'counts')\n",
        "\n",
        "#11/8: managing the imports\n",
        "#https://stackoverflow.com/questions/4383571/importing-files-from-different-folder\n",
        "import sys\n",
        "sys.path.insert(1, COUNTS_FOLDER)\n",
        "import word2counts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQqY2xdKrnUv"
      },
      "source": [
        "#Data and simulated child"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZM7rvor3MZK"
      },
      "source": [
        "class WordData():\n",
        "  \"\"\"\n",
        "  Stores and asks for data from the main word CSV.\n",
        "  \"\"\"\n",
        "  def __init__(self, data_path, data_folder):\n",
        "    \"\"\"\n",
        "    Assumes that the data-related calculations stored\n",
        "       as calculated by the word2counts.py file.\n",
        "\n",
        "    data_path (str) should point to the main words CSV\n",
        "    data_folder (str) should point to the \"counts\" folder.\n",
        "    \"\"\"\n",
        "    #Indexing for pg is determined by this function.\n",
        "    self.pg_idx = np.load(join(data_folder, 'pg_idx.npy'))\n",
        "    #The indexing here is (by words (parallel to main CSV), by pg pair).\n",
        "    self.counts = np.load(join(data_folder, 'counts.npy'))\n",
        "\n",
        "    self.word_csv = pd.read_csv(data_path)\n",
        "    word_freqs = self.word_csv['Frequency'].to_numpy()\n",
        "\n",
        "    self.word_freq_distr = Categorical(probs = torch.from_numpy(word_freqs))\n",
        "\n",
        "  def get_pg_len(self):\n",
        "    return self.pg_idx.shape[0]\n",
        "\n",
        "  def get_words_len(self):\n",
        "    return self.word_csv.shape[0]\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    \"\"\"\n",
        "    Gives CSV information, an entry/entries in the DataFrame.\n",
        "    Can index with either an integer or a list of integers.\n",
        "    \"\"\"\n",
        "    if isinstance(idx, int):\n",
        "      return self.word_csv.iloc[idx]\n",
        "    if isinstance(idx, list):\n",
        "      return self.word_csv[idx]\n",
        "\n",
        "  def get_counts(self, idx):\n",
        "    \"\"\"\n",
        "    Gives count information, a Numpy array.\n",
        "    \"\"\"\n",
        "    return self.counts[idx]\n",
        "\n",
        "  def sample_and_extract(self, num_samples):\n",
        "    \"\"\"\n",
        "    Samples num_samples (int) words according the word probabilities\n",
        "      and returns two things:\n",
        "        (1) this_words_idx, array, the words (in CSV indexing) selected to be learned\n",
        "        (2) the sum of pg counts, array, from the words,\n",
        "              to be used in updating child's exposure.\n",
        "    \"\"\"\n",
        "\n",
        "    #11/22: https://pytorch.org/docs/stable/tensors.html\n",
        "    this_words_idx = self.word_freq_distr.sample(sample_shape = torch.Size([num_samples])).numpy()\n",
        "    pg_update_counts = np.take(self.counts, this_words_idx, axis = 0)\n",
        "\n",
        "    return this_words_idx, np.sum(pg_update_counts, axis = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmQxQOTjmiv3"
      },
      "source": [
        "class Child():\n",
        "  \"\"\"\n",
        "  Represents a child.\n",
        "    Translates and is based on Nikasha's WebPPL program.\n",
        "  Attributes:\n",
        "    (1) lr, the learning rate of a child (alpha)\n",
        "    (2) exposures, Tensor, the count of fictional exposures to certain pg pairs.\n",
        "    (3) self.data, a pointer to the main words CSV file.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, data):\n",
        "    \"\"\"\n",
        "    Inputs: data, a WordData object with word and count information.\n",
        "    \"\"\"\n",
        "    self.data = data\n",
        "    self.lr = Uniform(0, 1).sample()\n",
        "    self.exposures = torch.zeros(self.data.get_pg_len(), dtype = int)\n",
        "    #TODO: Ask which mean, stdev\n",
        "    log_mean = np.log(self.data.get_words_len()//2)\n",
        "    self.fictional_exposure_distr = LogNormal(log_mean, 0.1)\n",
        "    #Indexing is in order of that of pg_idx.npy\n",
        "\n",
        "  def learn_words(self):\n",
        "    \"\"\"\n",
        "    Simulates one instance of exposure to words.\n",
        "    Returns a Tensor of effective exposures over all pg pairs.\n",
        "    \"\"\"\n",
        "    #Sample and \"learn\" new words\n",
        "\n",
        "    this_fictional_exposure = round(self.fictional_exposure_distr.sample().item())\n",
        "    word_sample_idxs, pg_update_counts = self.data.sample_and_extract(this_fictional_exposure)\n",
        "\n",
        "    self.update_fictional_exposure(pg_update_counts)\n",
        "\n",
        "    #Now, return the actual non-deterministic exposure\n",
        "    this_effective_exposures = self.give_effective_exposure()\n",
        "    return this_effective_exposures\n",
        "\n",
        "  def update_fictional_exposure(self, pg_update_counts):\n",
        "    \"\"\"\n",
        "    Increases fictional exposure for all pg pairs encountered.\n",
        "    Inputs: pg_update_counts, the second result of sample_and_extract\n",
        "    Returns: None, but mutates exposures\n",
        "    \"\"\"\n",
        "    self.exposures += pg_update_counts\n",
        "\n",
        "  def give_effective_exposure(self):\n",
        "    \"\"\"\n",
        "    Returns Tensor, the effective exposure for each pg pair,\n",
        "      which is the sum of fictional exposure number of LogNormal\n",
        "        with mean = 1, stdev = 1.\n",
        "    \"\"\"\n",
        "\n",
        "    #Ask if can represent distribution with non-one mean/stdev.\n",
        "    #TODO: vectorize more?\n",
        "\n",
        "    this_exposures = torch.zeros((self.data.get_pg_len()))\n",
        "\n",
        "    for idx, exposure in enumerate(self.exposures):\n",
        "      exposure = exposure.item()\n",
        "      mean = torch.ones((exposure,))\n",
        "      stdev = torch.ones(mean.shape) #TODO: ask for which stdev\n",
        "      this_distr = LogNormal(mean, stdev)\n",
        "      this_raw_exposures = this_distr.sample()\n",
        "\n",
        "      this_exposures[idx] = this_raw_exposures.sum()\n",
        "      #Across the exposure random variables.\n",
        "      \n",
        "    return this_exposures"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xn_VSp4MHBmT"
      },
      "source": [
        "#Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQU21KzrHAqi"
      },
      "source": [
        "MAIN_DATA = WordData(DATA_PATH, COUNTS_FOLDER)\n",
        "test_child = Child(MAIN_DATA)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}